#Time Series Analysis
setwd('D:\\R Studio')

# Load necessary libraries
library(tidyverse)
library(lubridate)
library(ggplot2)
library(zoo)   # For interpolation
library(caret) # For data splitting
library(gridExtra) # For combining plots
install.packages("imputeTS")
install.packages("forecast")
library(imputeTS)
library(forecast)
library(dplyr)

data=read.csv('SUZLON.csv')

# Convert 'Date' to Date type
data$Date <- as.Date(data$Date, format = "%Y-%m-%d")

# Check for missing dates and remove rows with missing dates
data <- data %>% drop_na(Date)

count_missing_values <- function(df) {
  sapply(df, function(x) sum(is.na(x) | x == "" | trimws(x) == ""))
}

# Count missing values in the dataset
missing_values <- count_missing_values(data)
cat("Missing values in the dataset:\n")
print(missing_values)

head(data)

sum(is.na(data))

ggplot(data, aes(x = Date, y = Close)) +
  geom_line() +
  labs(title = "SUZLON closing Price", x = "Date", y = "Price")

# Split the data into training and testing sets
split_date <- as.Date("2023-12-31")
train_data <- data %>% filter(Date <= split_date)
test_data <- data %>% filter(Date > split_date)

# Convert the data to monthly
data_monthly <- data %>%
  group_by(month = floor_date(Date, "month")) %>%
  summarise(Close = mean(Close))

# Create time series object
ts <- ts(data_monthly$Close, start = c(2020, 1), frequency = 12)

# Decompose the time series using additive model
decomp_additive <- decompose(ts, type = "additive")

# Decompose the time series using multiplicative model
decomp_multiplicative <- decompose(ts, type = "multiplicative")

# Plot the decomposed components for additive model
autoplot(decomp_additive) +
  ggtitle("Additive Decomposition of SUZLON Stock Price") +
  theme_minimal()

# Plot the decomposed components for multiplicative model
autoplot(decomp_multiplicative) +
  ggtitle("Multiplicative Decomposition of SUZLON Stock Price") +
  theme_minimal()


## UNIVARIATE ANALYSIS

# Create time series objects
ts_daily <- ts(data$Close, start = c(2020, 1), frequency = 365.25)
ts_monthly <- ts(data_monthly$Close, start = c(2020, 1), frequency = 12)

# 1. Holt-Winters model and forecast for the next year
hw_model <- HoltWinters(ts_monthly)
hw_forecast <- forecast(hw_model, h = 12)
autoplot(hw_forecast) +
  ggtitle("Holt-Winters Forecast for SUZLON Stock Price") +
  theme_minimal()

# 2. Fit ARIMA model to the daily data
arima_model_daily <- auto.arima(ts_daily)
summary(arima_model_daily)

# Diagnostic check for ARIMA model
checkresiduals(arima_model_daily)

# Fit SARIMA model to the daily data
sarima_model_daily <- auto.arima(ts_daily, seasonal = TRUE)
summary(sarima_model_daily)

# Compare ARIMA and SARIMA models
arima_aic <- AIC(arima_model_daily)
sarima_aic <- AIC(sarima_model_daily)
print(paste("ARIMA AIC:", arima_aic))
print(paste("SARIMA AIC:", sarima_aic))

# Forecast the series for the next three months using the better model
best_model_daily <- ifelse(arima_aic < sarima_aic, arima_model_daily, sarima_model_daily)
daily_forecast <- forecast(best_model_daily, h = 90)
autoplot(daily_forecast) +
  ggtitle("Daily Forecast for SUZLON Stock Price") +
  theme_minimal()

# 3. Fit ARIMA model to the monthly series
arima_model_monthly <- auto.arima(ts_monthly)
summary(arima_model_monthly)

# Forecast the monthly series
monthly_forecast <- forecast(arima_model_monthly, h = 12)
autoplot(monthly_forecast) +
  ggtitle("Monthly ARIMA Forecast for SUZLON Stock Price") +
  theme_minimal()

## MULTIVARIATE
install.packages("keras")
install.packages("randomForest")
library(keras)
library(randomForest)
library(rpart)
library(caret)

# Feature engineering: Create lagged variables and moving averages as features
data <- data %>%
  mutate(Price_lag1 = lag(Close, 1),
         Price_lag2 = lag(Close, 2),
         Price_lag3 = lag(Close, 3),
         Vol_lag1 = lag(Volume, 1),
         Vol_lag2 = lag(Volume, 2),
         Vol_lag3 = lag(Volume, 3),
         Price_ma7 = rollmean(Close, 7, fill = NA, align = "right"),
         Price_ma30 = rollmean(Close, 30, fill = NA, align = "right"))

# Remove rows with NA values generated by lagging
data <-data %>% drop_na()

# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data$Close, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Convert features to numeric
train_features <- train_data %>% select(-Date, -Close) %>% mutate(across(everything(), as.numeric))
test_features <- test_data %>% select(-Date, -Close) %>% mutate(across(everything(), as.numeric))

# Check if the number of columns match
if (ncol(train_features) != ncol(test_features)) {
  stop("The number of columns in train_features and test_features do not match.")
}

# Normalize the features
train_features <- scale(train_features)
test_features <- scale(test_features, center = attr(train_features, "scaled:center"), scale = attr(train_features, "scaled:scale"))

# Prepare labels
train_labels <- train_data$Close
test_labels <- test_data$Close

# Reshape the data for LSTM input (samples, time steps, features)
train_array <- array(train_features, dim = c(nrow(train_features), 1, ncol(train_features)))
test_array <- array(test_features, dim = c(nrow(test_features), 1, ncol(test_features)))

# Build and train the LSTM model
lstm_model <- keras_model_sequential() %>% 
  layer_lstm(units = 50, input_shape = c(1, ncol(train_features)), return_sequences = TRUE) %>%
  layer_lstm(units = 50) %>%
  layer_dense(units = 1)

lstm_model %>% 
  compile(
    loss = 'mean_squared_error',
    optimizer = 'adam'
  )

history <- lstm_model %>% fit(
  train_array, train_labels,
  epochs = 50,
  batch_size = 32,
  validation_split = 0.2
)
install.packages("tensorflow") 

# Forecast using the LSTM model
lstm_forecast <- lstm_model %>% predict(test_array)

# Plot the LSTM forecast
plot(test_labels, type = "l", col = "blue", main = "LSTM Forecast vs Actual", xlab = "Time", ylab = "Close")
lines(lstm_forecast, col = "red")
legend("topright", legend = c("Actual", "Forecast"), col = c("blue", "red"), lty = 1)

# Build and train the Random Forest model
rf_model <- randomForest(Close ~ ., data = train_data %>% select(-Date))
rf_forecast <- predict(rf_model, test_data %>% select(-Date, -Close))

# Plot the Random Forest forecast
plot(test_labels, type = "l", col = "blue", main = "Random Forest Forecast vs Actual", xlab = "Time", ylab = "Close")
lines(rf_forecast, col = "red")
legend("topright", legend = c("Actual", "Forecast"), col = c("blue", "red"), lty = 1)

# Build and train the Decision Tree model
dt_model <- rpart(Close ~ ., data = train_data %>% select(-Date))
dt_forecast <- predict(dt_model, test_data %>% select(-Date, -Close))

# Plot the Decision Tree forecast
plot(test_labels, type = "l", col = "blue", main = "Decision Tree Forecast vs Actual", xlab = "Time", ylab = "Close")
lines(dt_forecast, col = "red")
legend("topright", legend = c("Actual", "Forecast"), col = c("blue", "red"), lty = 1)
